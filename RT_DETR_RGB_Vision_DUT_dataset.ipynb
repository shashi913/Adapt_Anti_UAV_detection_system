{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz4xOlF0n0fR",
        "outputId": "0b187d42-1285-47b5-e7c4-97500e3025c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall incompatible versions\n",
        "!pip uninstall --yes torch torchvision torchaudio -q"
      ],
      "metadata": {
        "id": "KgNa1Y7pz4Fx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install compatible PyTorch 2.0.1 with CUDA 11.7\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu117 -q\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install onnx==1.14.0 onnxruntime==1.15.1 pycocotools pyyaml scipy transformers -q\n",
        "\n",
        "# CRITICAL: Fix NumPy version\n",
        "!pip install numpy==1.23.5 -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGn5yZx1z98F",
        "outputId": "edec6e6c-bf3f-44c8-cc0c-cdb861403dd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "onnxruntime 1.15.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.10.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvoOSUn9vPxx",
        "outputId": "7f11a119-10be-4f63-a2b7-d88775e138fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# 1. Verify GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXEMfyqWLTag",
        "outputId": "8ffbd59f-27db-4aca-b9f4-412fac18226d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clone the official repository into the local Colab space (faster than Drive)\n",
        "%cd /content/\n",
        "!git clone https://github.com/lyuwenyu/RT-DETR.git\n",
        "\n",
        "# 2. Navigate to the PyTorch implementation folder\n",
        "%cd /content/RT-DETR/rtdetr_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAdAjDwoyRBn",
        "outputId": "58bebd50-a5ff-4545-9eda-674d8ceeab97",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'RT-DETR' already exists and is not an empty directory.\n",
            "/content/RT-DETR/rtdetr_pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiPoQMTHZeF1",
        "outputId": "7a8f92c3-14ca-46da-829e-c161fb4bd8b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torchdata 0.11.0\n",
            "Uninstalling torchdata-0.11.0:\n",
            "  Successfully uninstalled torchdata-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install compatible dependencies\n",
        "# # The RT-DETR repo requires specific versions, but we'll use newer compatible ones\n",
        "# !pip install -q pycocotools scipy pyyaml opencv-python tqdm"
      ],
      "metadata": {
        "id": "vZa7GWRjQagV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. Install dependencies\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "b6DooN9ALd0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e57b29-07c5-435c-cb5d-9d4951b79def"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu117)\n",
            "Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.15.2+cu117)\n",
            "Requirement already satisfied: onnx==1.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.14.0)\n",
            "Requirement already satisfied: onnxruntime==1.15.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.15.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.0.10)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.15.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2->-r requirements.txt (line 2)) (2.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2->-r requirements.txt (line 2)) (11.2.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx==1.14.0->-r requirements.txt (line 3)) (5.29.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.15.1->-r requirements.txt (line 4)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.15.1->-r requirements.txt (line 4)) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.15.1->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (15.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.33.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 8)) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r requirements.txt (line 8)) (1.1.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime==1.15.1->-r requirements.txt (line 4)) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2->-r requirements.txt (line 2)) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check current PyTorch version\n",
        "import torch\n",
        "print(f\"Current PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKQVpgN8QV7P",
        "outputId": "2800d903-796b-4256-a6e2-453e6ec5ff72",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current PyTorch version: 2.0.1+cu117\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract datasets from Google Drive to local storage (faster)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "drive_base = '/content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT'\n",
        "datasets = {'train': 'train.zip', 'val': 'val.zip', 'test': 'test.zip'}\n",
        "\n",
        "for split, zip_file in datasets.items():\n",
        "    zip_path = f'{drive_base}/{zip_file}'\n",
        "    local_zip = f'/content/DUT-{split}.zip'\n",
        "    local_extract = f'/content/DUT-{split}'\n",
        "\n",
        "    if os.path.exists(zip_path) and not os.path.exists(local_extract):\n",
        "        print(f\"üì¶ Extracting {split}...\")\n",
        "        shutil.copy(zip_path, local_zip)\n",
        "        !unzip -q {local_zip} -d {local_extract}\n",
        "        os.remove(local_zip)\n",
        "\n",
        "print(\"‚úÖ Datasets extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t38lXS-GMZRk",
        "outputId": "dc1894f6-9053-4a60-91a4-316702d2cf02"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify XML structure\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "test_xml_dir = '/content/DUT-train/train/xml'\n",
        "sample_files = os.listdir(test_xml_dir)[:3]\n",
        "\n",
        "print(\"Checking sample XML files:\\n\")\n",
        "for sample_xml in sample_files:\n",
        "    tree = ET.parse(os.path.join(test_xml_dir, sample_xml))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    print(f\"File: {sample_xml}\")\n",
        "    for obj in root.findall('object'):\n",
        "        name = obj.find('name').text\n",
        "        bndbox = obj.find('bndbox')\n",
        "        print(f\"  Object: {name}, BBox: [{bndbox.find('xmin').text}, {bndbox.find('ymin').text}, {bndbox.find('xmax').text}, {bndbox.find('ymax').text}]\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXcz50TLMok-",
        "outputId": "60b47f9d-b21f-42cf-d274-7881a05f0101"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking sample XML files:\n",
            "\n",
            "File: 03094.xml\n",
            "  Object: UAV, BBox: [1178, 698, 1237, 745]\n",
            "\n",
            "File: 02380.xml\n",
            "  Object: UAV, BBox: [917, 483, 955, 512]\n",
            "\n",
            "File: 04691.xml\n",
            "  Object: UAV, BBox: [857, 598, 873, 603]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert XML to COCO Format\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "CLASS_MAP = {'UAV': 1}\n",
        "\n",
        "def convert_xml_to_coco(image_dir, xml_dir, output_file, split_name):\n",
        "    \"\"\"Convert Pascal VOC XML annotations to COCO JSON format\"\"\"\n",
        "\n",
        "    images = []\n",
        "    annotations = []\n",
        "    categories = [{\"id\": 1, \"name\": \"uav\", \"supercategory\": \"vehicle\"}]\n",
        "\n",
        "    annotation_id = 1\n",
        "    image_id = 1\n",
        "\n",
        "    print(f\"\\nüîÑ Converting {split_name} dataset...\")\n",
        "    print(f\"  Images: {image_dir}\")\n",
        "    print(f\"  XMLs: {xml_dir}\")\n",
        "\n",
        "    if not os.path.exists(image_dir) or not os.path.exists(xml_dir):\n",
        "        print(f\"  ‚ùå Directory not found!\")\n",
        "        return\n",
        "\n",
        "    # Get all image files\n",
        "    valid_images = [f for f in os.listdir(image_dir)\n",
        "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    print(f\"  Found {len(valid_images)} images\")\n",
        "\n",
        "    skipped = 0\n",
        "\n",
        "    for filename in tqdm(valid_images, desc=f\"Processing {split_name}\"):\n",
        "        xml_filename = os.path.splitext(filename)[0] + '.xml'\n",
        "        xml_path = os.path.join(xml_dir, xml_filename)\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "\n",
        "        if not os.path.exists(xml_path):\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Parse XML\n",
        "        try:\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "        except ET.ParseError:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Get image dimensions\n",
        "        size_node = root.find('size')\n",
        "        if size_node is not None:\n",
        "            width = int(size_node.find('width').text)\n",
        "            height = int(size_node.find('height').text)\n",
        "        else:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            height, width = img.shape[:2]\n",
        "\n",
        "        images.append({\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": filename,\n",
        "            \"height\": height,\n",
        "            \"width\": width\n",
        "        })\n",
        "\n",
        "        # Extract annotations\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            class_id = CLASS_MAP.get(name)\n",
        "\n",
        "            if class_id is None:\n",
        "                continue\n",
        "\n",
        "            bndbox = obj.find('bndbox')\n",
        "            xmin = float(bndbox.find('xmin').text)\n",
        "            ymin = float(bndbox.find('ymin').text)\n",
        "            xmax = float(bndbox.find('xmax').text)\n",
        "            ymax = float(bndbox.find('ymax').text)\n",
        "\n",
        "            bbox_width = xmax - xmin\n",
        "            bbox_height = ymax - ymin\n",
        "\n",
        "            if bbox_width <= 0 or bbox_height <= 0:\n",
        "                continue\n",
        "\n",
        "            annotations.append({\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": class_id,\n",
        "                \"bbox\": [xmin, ymin, bbox_width, bbox_height],\n",
        "                \"area\": bbox_width * bbox_height,\n",
        "                \"iscrowd\": 0\n",
        "            })\n",
        "            annotation_id += 1\n",
        "\n",
        "        image_id += 1\n",
        "\n",
        "    # Create COCO format dict\n",
        "    coco_format = {\n",
        "        \"images\": images,\n",
        "        \"annotations\": annotations,\n",
        "        \"categories\": categories\n",
        "    }\n",
        "\n",
        "    # Save JSON\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_format, f)\n",
        "\n",
        "    print(f\"  ‚úì {len(images)} images, {len(annotations)} annotations\")\n",
        "    if skipped > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  Skipped {skipped} files\")\n",
        "    print(f\"  Saved to: {output_file}\\n\")\n",
        "\n",
        "# Convert all datasets\n",
        "base_dir = '/content'\n",
        "for split in ['train', 'val', 'test']:\n",
        "    dataset_dir = f'{base_dir}/DUT-{split}/{split}'\n",
        "\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"‚ö†Ô∏è  Skipping {split} - not found\")\n",
        "        continue\n",
        "\n",
        "    convert_xml_to_coco(\n",
        "        image_dir=f'{dataset_dir}/img',\n",
        "        xml_dir=f'{dataset_dir}/xml',\n",
        "        output_file=f'{base_dir}/DUT-{split}/annotations/{split}.json',\n",
        "        split_name=split\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ All datasets converted to COCO format!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suo8dFTUM6l8",
        "outputId": "ab1e5011-9d59-4d0e-b85a-ab5d151f3980"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Converting train dataset...\n",
            "  Images: /content/DUT-train/train/img\n",
            "  XMLs: /content/DUT-train/train/xml\n",
            "  Found 5200 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5200/5200 [00:00<00:00, 12770.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì 5200 images, 5243 annotations\n",
            "  Saved to: /content/DUT-train/annotations/train.json\n",
            "\n",
            "\n",
            "üîÑ Converting val dataset...\n",
            "  Images: /content/DUT-val/val/img\n",
            "  XMLs: /content/DUT-val/val/xml\n",
            "  Found 2600 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2600/2600 [00:00<00:00, 13672.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì 2600 images, 2620 annotations\n",
            "  Saved to: /content/DUT-val/annotations/val.json\n",
            "\n",
            "\n",
            "üîÑ Converting test dataset...\n",
            "  Images: /content/DUT-test/test/img\n",
            "  XMLs: /content/DUT-test/test/xml\n",
            "  Found 2200 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2200/2200 [00:00<00:00, 13493.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì 2200 images, 2245 annotations\n",
            "  Saved to: /content/DUT-test/annotations/test.json\n",
            "\n",
            "‚úÖ All datasets converted to COCO format!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify COCO format\n",
        "import json\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    json_path = f'/content/DUT-{split}/annotations/{split}.json'\n",
        "\n",
        "    if not os.path.exists(json_path):\n",
        "        continue\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"\\n{split.upper()} Dataset:\")\n",
        "    print(f\"  Images: {len(data['images'])}\")\n",
        "    print(f\"  Annotations: {len(data['annotations'])}\")\n",
        "    print(f\"  Categories: {data['categories']}\")\n",
        "\n",
        "    if len(data['annotations']) > 0:\n",
        "        sample = data['annotations'][0]\n",
        "        print(f\"  Sample annotation: {sample}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IvvHZi2SDTL",
        "outputId": "371bacac-3026-456b-a8f6-297e5179b400",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TRAIN Dataset:\n",
            "  Images: 5200\n",
            "  Annotations: 5243\n",
            "  Categories: [{'id': 1, 'name': 'uav', 'supercategory': 'vehicle'}]\n",
            "  Sample annotation: {'id': 1, 'image_id': 1, 'category_id': 1, 'bbox': [1073.0, 664.0, 44.0, 22.0], 'area': 968.0, 'iscrowd': 0}\n",
            "\n",
            "VAL Dataset:\n",
            "  Images: 2600\n",
            "  Annotations: 2620\n",
            "  Categories: [{'id': 1, 'name': 'uav', 'supercategory': 'vehicle'}]\n",
            "  Sample annotation: {'id': 1, 'image_id': 1, 'category_id': 1, 'bbox': [546.0, 417.0, 38.0, 33.0], 'area': 1254.0, 'iscrowd': 0}\n",
            "\n",
            "TEST Dataset:\n",
            "  Images: 2200\n",
            "  Annotations: 2245\n",
            "  Categories: [{'id': 1, 'name': 'uav', 'supercategory': 'vehicle'}]\n",
            "  Sample annotation: {'id': 1, 'image_id': 1, 'category_id': 1, 'bbox': [650.0, 385.0, 38.0, 30.0], 'area': 1140.0, 'iscrowd': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/RT-DETR/rtdetr_pytorch/configs/rtdetr/rtdetr_r18vd_uav.yml\n",
        "\n",
        "__include__: [\n",
        "  '../dataset/coco_detection.yml',\n",
        "  '../runtime.yml',\n",
        "  './include/rtdetr_r18vd.yml',\n",
        "]\n",
        "\n",
        "# Override num_classes for single class (UAV)\n",
        "num_classes: 1\n",
        "\n",
        "# Training configuration\n",
        "epoches: 72\n",
        "use_amp: True\n",
        "output_dir: '/content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT/output'\n",
        "\n",
        "# Optimizer\n",
        "optimizer:\n",
        "  type: AdamW\n",
        "  params:\n",
        "    -\n",
        "      params: '^(?=.*backbone)(?=.*norm).*$'\n",
        "      lr: 0.00001\n",
        "      weight_decay: 0.\n",
        "    -\n",
        "      params: '^(?=.*backbone)(?!.*norm).*$'\n",
        "      lr: 0.00001\n",
        "    -\n",
        "      params: '^(?!.*backbone).*$'\n",
        "      lr: 0.0001\n",
        "  lr: 0.0001\n",
        "  betas: [0.9, 0.999]\n",
        "  weight_decay: 0.0001\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler:\n",
        "  type: MultiStepLR\n",
        "  milestones: [60]\n",
        "  gamma: 0.1\n",
        "\n",
        "# LR warmup\n",
        "lr_warmup_scheduler:\n",
        "  type: LinearWarmup\n",
        "  warmup_duration: 1000\n",
        "\n",
        "# Training dataloader\n",
        "train_dataloader:\n",
        "  type: DataLoader\n",
        "  dataset:\n",
        "    type: CocoDetection\n",
        "    img_folder: '/content/DUT-train/train/img'\n",
        "    ann_file: '/content/DUT-train/annotations/train.json'\n",
        "    return_masks: False\n",
        "    transforms:\n",
        "      type: Compose\n",
        "      ops:\n",
        "        - {type: RandomPhotometricDistort, p: 0.5}\n",
        "        - {type: RandomZoomOut, p: 0.5}\n",
        "        - {type: RandomIoUCrop, p: 0.8}\n",
        "        - {type: SanitizeBoundingBox}\n",
        "        - {type: RandomHorizontalFlip}\n",
        "        - {type: Resize, size: [640, 640]}\n",
        "        - {type: ToTensor}\n",
        "        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}\n",
        "  shuffle: True\n",
        "  batch_size: 16\n",
        "  num_workers: 4\n",
        "  drop_last: True\n",
        "  collate_fn:\n",
        "    type: BatchImageCollateFuncion\n",
        "\n",
        "# Validation dataloader\n",
        "val_dataloader:\n",
        "  type: DataLoader\n",
        "  dataset:\n",
        "    type: CocoDetection\n",
        "    img_folder: '/content/DUT-val/val/img'\n",
        "    ann_file: '/content/DUT-val/annotations/val.json'\n",
        "    return_masks: False\n",
        "    transforms:\n",
        "      type: Compose\n",
        "      ops:\n",
        "        - {type: Resize, size: [640, 640]}\n",
        "        - {type: ToTensor}\n",
        "        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}\n",
        "  shuffle: False\n",
        "  batch_size: 16\n",
        "  num_workers: 4\n",
        "  drop_last: False\n",
        "  collate_fn:\n",
        "    type: BatchImageCollateFuncion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dO7gUSfSIP7",
        "outputId": "71ecbc41-8084-4350-a8e3-08cfdd346bad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/RT-DETR/rtdetr_pytorch/configs/rtdetr/rtdetr_r18vd_uav.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify config file\n",
        "!cat /content/RT-DETR/rtdetr_pytorch/configs/rtdetr/rtdetr_r18vd_uav.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzCNDlhbSZ-5",
        "outputId": "b763574b-4289-49b2-e183-cecd362bdd56",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "__include__: [\n",
            "  '../dataset/coco_detection.yml',\n",
            "  '../runtime.yml',\n",
            "  './include/rtdetr_r18vd.yml',\n",
            "]\n",
            "\n",
            "# Override num_classes for single class (UAV)\n",
            "num_classes: 1\n",
            "\n",
            "# Training configuration\n",
            "epoches: 72\n",
            "use_amp: True\n",
            "output_dir: '/content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT/output'\n",
            "\n",
            "# Optimizer\n",
            "optimizer:\n",
            "  type: AdamW\n",
            "  params:\n",
            "    -\n",
            "      params: '^(?=.*backbone)(?=.*norm).*$'\n",
            "      lr: 0.00001\n",
            "      weight_decay: 0.\n",
            "    -\n",
            "      params: '^(?=.*backbone)(?!.*norm).*$'\n",
            "      lr: 0.00001\n",
            "    -\n",
            "      params: '^(?!.*backbone).*$'\n",
            "      lr: 0.0001\n",
            "  lr: 0.0001\n",
            "  betas: [0.9, 0.999]\n",
            "  weight_decay: 0.0001\n",
            "\n",
            "# Learning rate scheduler\n",
            "lr_scheduler:\n",
            "  type: MultiStepLR\n",
            "  milestones: [60]\n",
            "  gamma: 0.1\n",
            "\n",
            "# LR warmup\n",
            "lr_warmup_scheduler:\n",
            "  type: LinearWarmup\n",
            "  warmup_duration: 1000\n",
            "\n",
            "# Training dataloader\n",
            "train_dataloader:\n",
            "  type: DataLoader\n",
            "  dataset:\n",
            "    type: CocoDetection\n",
            "    img_folder: '/content/DUT-train/train/img'\n",
            "    ann_file: '/content/DUT-train/annotations/train.json'\n",
            "    return_masks: False\n",
            "    transforms:\n",
            "      type: Compose\n",
            "      ops:\n",
            "        - {type: RandomPhotometricDistort, p: 0.5}\n",
            "        - {type: RandomZoomOut, p: 0.5}\n",
            "        - {type: RandomIoUCrop, p: 0.8}\n",
            "        - {type: SanitizeBoundingBox}\n",
            "        - {type: RandomHorizontalFlip}\n",
            "        - {type: Resize, size: [640, 640]}\n",
            "        - {type: ToTensor}\n",
            "        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}\n",
            "  shuffle: True\n",
            "  batch_size: 16\n",
            "  num_workers: 4\n",
            "  drop_last: True\n",
            "  collate_fn:\n",
            "    type: BatchImageCollateFuncion\n",
            "\n",
            "# Validation dataloader\n",
            "val_dataloader:\n",
            "  type: DataLoader\n",
            "  dataset:\n",
            "    type: CocoDetection\n",
            "    img_folder: '/content/DUT-val/val/img'\n",
            "    ann_file: '/content/DUT-val/annotations/val.json'\n",
            "    return_masks: False\n",
            "    transforms:\n",
            "      type: Compose\n",
            "      ops:\n",
            "        - {type: Resize, size: [640, 640]}\n",
            "        - {type: ToTensor}\n",
            "        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}\n",
            "  shuffle: False\n",
            "  batch_size: 16\n",
            "  num_workers: 4\n",
            "  drop_last: False\n",
            "  collate_fn:\n",
            "    type: BatchImageCollateFuncion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Final verification\n",
        "# import subprocess\n",
        "# import sys\n",
        "\n",
        "# test_script = \"\"\"\n",
        "# import sys\n",
        "# sys.path.insert(0, '/content/RT-DETR/rtdetr_pytorch')\n",
        "\n",
        "# try:\n",
        "#     print(\"üîç Final Import Test\\\\n\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     print(\"\\\\n1. Testing coco_dataset...\")\n",
        "#     from src.data.coco.coco_dataset import CocoDetection\n",
        "#     print(\"   ‚úì coco_dataset imported successfully\")\n",
        "\n",
        "#     print(\"\\\\n2. Testing transforms...\")\n",
        "#     from src.data.transforms import RandomPhotometricDistort, ToImage\n",
        "#     print(\"   ‚úì transforms imported successfully\")\n",
        "\n",
        "#     print(\"\\\\n3. Testing training module...\")\n",
        "#     import src.misc.dist as dist\n",
        "#     print(\"   ‚úì dist module imported successfully\")\n",
        "\n",
        "#     print(\"\\\\n\" + \"=\"*70)\n",
        "#     print(\"üéâüéâüéâ ALL IMPORTS WORKING! üéâüéâüéâ\")\n",
        "#     print(\"RT-DETR is now ready to train!\")\n",
        "#     print(\"=\"*70)\n",
        "#     exit(0)\n",
        "\n",
        "# except Exception as e:\n",
        "#     print(f\"\\\\n‚ùå Still failing: {e}\")\n",
        "#     import traceback\n",
        "#     traceback.print_exc()\n",
        "#     exit(1)\n",
        "# \"\"\"\n",
        "\n",
        "# result = subprocess.run([sys.executable, '-c', test_script], capture_output=True, text=True)\n",
        "# print(result.stdout)\n",
        "# if result.returncode != 0:\n",
        "#     print(result.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC2whDYUYMUh",
        "outputId": "1264d799-f0f5-4358-8815-be1815106b9f",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Final Import Test\n",
            "\n",
            "======================================================================\n",
            "\n",
            "1. Testing coco_dataset...\n",
            "   ‚úì coco_dataset imported successfully\n",
            "\n",
            "2. Testing transforms...\n",
            "\n",
            "‚ùå Still failing: cannot import name 'ToImage' from 'src.data.transforms' (/content/RT-DETR/rtdetr_pytorch/src/data/transforms.py)\n",
            "\n",
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 14, in <module>\n",
            "ImportError: cannot import name 'ToImage' from 'src.data.transforms' (/content/RT-DETR/rtdetr_pytorch/src/data/transforms.py)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "%cd /content/RT-DETR/rtdetr_pytorch\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT/output'\n",
        "\n",
        "!python tools/train.py \\\n",
        "    -c configs/rtdetr/rtdetr_r18vd_uav.yml \\\n",
        "    --amp \\\n",
        "    --seed 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0deFg3DPSfSv",
        "outputId": "e53d90c3-cc9b-4863-fa8d-1c0394eeaa47",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RT-DETR/rtdetr_pytorch\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 9, in <module>\n",
            "    import src.misc.dist as dist\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/__init__.py\", line 2, in <module>\n",
            "    from . import data\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/__init__.py\", line 2, in <module>\n",
            "    from .coco import *\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/coco/__init__.py\", line 1, in <module>\n",
            "    from .coco_dataset import (\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/coco/coco_dataset.py\", line 11, in <module>\n",
            "    import torchvision\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import datasets, io, models, ops, transforms, utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
            "    from .anchor_utils import AnchorGenerator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
            "    class AnchorGenerator(nn.Module):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
            "    device: torch.device = torch.device(\"cpu\"),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n",
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1+cu117\n",
            "Not init distributed mode.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 50, in <module>\n",
            "    main(args)\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 24, in main\n",
            "    cfg = YAMLConfig(\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/core/yaml_config.py\", line 18, in __init__\n",
            "    cfg = load_config(cfg_path)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/core/yaml_utils.py\", line 172, in load_config\n",
            "    with open(base_yaml) as f:\n",
            "         ^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'configs/rtdetr/./include/rtdetr_r18vd.yml'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/RT-DETR/rtdetr_pytorch')\n",
        "output_dir = '/content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT/output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ Starting RT-DETR Training\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "!python tools/train.py \\\n",
        "    -c configs/rtdetr/rtdetr_r18vd_uav.yml \\\n",
        "    --seed 0\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzxlUm-36CxU",
        "outputId": "abe5fb9d-3f55-44f1-a950-27aec6da2b70"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üöÄ Starting RT-DETR Training\n",
            "======================================================================\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 9, in <module>\n",
            "    import src.misc.dist as dist\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/__init__.py\", line 2, in <module>\n",
            "    from . import data\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/__init__.py\", line 2, in <module>\n",
            "    from .coco import *\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/coco/__init__.py\", line 1, in <module>\n",
            "    from .coco_dataset import (\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/coco/coco_dataset.py\", line 11, in <module>\n",
            "    import torchvision\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import datasets, io, models, ops, transforms, utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
            "    from .anchor_utils import AnchorGenerator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
            "    class AnchorGenerator(nn.Module):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
            "    device: torch.device = torch.device(\"cpu\"),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n",
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1+cu117\n",
            "Not init distributed mode.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 50, in <module>\n",
            "    main(args)\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 24, in main\n",
            "    cfg = YAMLConfig(\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/core/yaml_config.py\", line 18, in __init__\n",
            "    cfg = load_config(cfg_path)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/core/yaml_utils.py\", line 172, in load_config\n",
            "    with open(base_yaml) as f:\n",
            "         ^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'configs/rtdetr/./include/rtdetr_r18vd.yml'\n",
            "\n",
            "‚úÖ Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content/RT-DETR/rtdetr_pytorch/configs/rtdetr/include/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Aenmuy8UyNsF",
        "outputId": "c39797c0-19aa-43c8-f18f-073b0f259397"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 36\n",
            "drwxr-xr-x 2 root root 4096 Dec  2 10:03 .\n",
            "drwxr-xr-x 3 root root 4096 Dec  2 10:12 ..\n",
            "-rw-r--r-- 1 root root 1110 Dec  2 10:03 dataloader_regnet.yml\n",
            "-rw-r--r-- 1 root root 1110 Dec  2 10:03 dataloader.yml\n",
            "-rw-r--r-- 1 root root  478 Dec  2 10:03 optimizer_regnet.yml\n",
            "-rw-r--r-- 1 root root  528 Dec  2 10:03 optimizer.yml\n",
            "-rw-r--r-- 1 root root 1257 Dec  2 10:03 rtdetr_dla34.yml\n",
            "-rw-r--r-- 1 root root 1324 Dec  2 10:03 rtdetr_r50vd.yml\n",
            "-rw-r--r-- 1 root root 1258 Dec  2 10:03 rtdetr_regnet.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The output directory is set to your Drive folder\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT/output'\n",
        "\n",
        "!python tools/train.py \\\n",
        "    -c configs/rtdetr/rtdetr_r18vd_uav.yml \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --amp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sm2cNBG_fm8",
        "outputId": "167e453b-5456-4937-a363-0c56a5bde8de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/RT-DETR/rtdetr_pytorch/tools/train.py\", line 9, in <module>\n",
            "    import src.misc.dist as dist\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/__init__.py\", line 2, in <module>\n",
            "    from . import data\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/__init__.py\", line 2, in <module>\n",
            "    from .coco import *\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/coco/__init__.py\", line 1, in <module>\n",
            "    from .coco_dataset import (\n",
            "  File \"/content/RT-DETR/rtdetr_pytorch/tools/../src/data/coco/coco_dataset.py\", line 11, in <module>\n",
            "    import torchvision\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import datasets, io, models, ops, transforms, utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
            "    from .anchor_utils import AnchorGenerator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
            "    class AnchorGenerator(nn.Module):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
            "    device: torch.device = torch.device(\"cpu\"),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n",
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
            "usage: train.py [-h] [--config CONFIG] [--resume RESUME] [--tuning TUNING]\n",
            "                [--test-only] [--amp] [--seed SEED]\n",
            "train.py: error: unrecognized arguments: --output_dir /content/drive/MyDrive/FYP_datasets/Vision-RGB/DUT/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"Has datapoints: {hasattr(torchvision, 'datapoints')}\")\n",
        "print(f\"Has tv_tensors: {hasattr(torchvision, 'tv_tensors')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlLlkz-gWWmN",
        "outputId": "7c711df6-3de4-4bad-fd51-40223078bbdd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torchvision version: 0.15.2+cu117\n",
            "Has datapoints: False\n",
            "Has tv_tensors: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-19-1403011217.py\", line 1, in <cell line: 0>\n",
            "    import torchvision\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import datasets, io, models, ops, transforms, utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
            "    from .anchor_utils import AnchorGenerator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
            "    class AnchorGenerator(nn.Module):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
            "    device: torch.device = torch.device(\"cpu\"),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to see what's available in your torchvision\n",
        "import torchvision\n",
        "\n",
        "print(\"Available in torchvision:\")\n",
        "print([attr for attr in dir(torchvision) if not attr.startswith('_')])\n",
        "\n",
        "print(\"\\n\\nChecking tv_tensors location:\")\n",
        "try:\n",
        "    from torchvision import tv_tensors\n",
        "    print(f\"‚úì tv_tensors found at: {tv_tensors}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó tv_tensors not found: {e}\")\n",
        "\n",
        "print(\"\\nChecking datapoints:\")\n",
        "try:\n",
        "    from torchvision import datapoints\n",
        "    print(f\"‚úì datapoints found\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó datapoints not found: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGPFeD0wXLp0",
        "outputId": "89e16982-1cf7-4b45-93d8-b17e7a6d8263"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available in torchvision:\n",
            "['Module', 'datasets', 'disable_beta_transforms_warning', 'extension', 'get_image_backend', 'get_video_backend', 'io', 'models', 'ops', 'os', 'set_image_backend', 'set_video_backend', 'torch', 'transforms', 'utils', 'version', 'warnings']\n",
            "\n",
            "\n",
            "Checking tv_tensors location:\n",
            "‚úì tv_tensors found at: <module 'torchvision.tv_tensors' from '/usr/local/lib/python3.12/dist-packages/torchvision/tv_tensors/__init__.py'>\n",
            "\n",
            "Checking datapoints:\n",
            "‚úó datapoints not found: cannot import name 'datapoints' from 'torchvision' (/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick diagnostic\n",
        "try:\n",
        "    from torchvision import tv_tensors\n",
        "    print(\"‚úì tv_tensors IS available!\")\n",
        "    print(f\"  Location: {tv_tensors.__file__}\")\n",
        "    print(f\"  Contents: {dir(tv_tensors)[:10]}\")  # Show first 10 items\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó tv_tensors NOT available: {e}\")\n",
        "    print(\"\\nThis means we need a different approach!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhi9Bp9eXQfi",
        "outputId": "83a5fe26-465e-49c8-92ea-b1f0f24fbf61"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì tv_tensors IS available!\n",
            "  Location: /usr/local/lib/python3.12/dist-packages/torchvision/tv_tensors/__init__.py\n",
            "  Contents: ['BoundingBoxFormat', 'BoundingBoxes', 'Image', 'KeyPoints', 'Mask', 'TVTensor', 'Video', '__builtins__', '__cached__', '__doc__']\n"
          ]
        }
      ]
    }
  ]
}